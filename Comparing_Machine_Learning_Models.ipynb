{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1OoCaJV2V1zsG3X/gtdYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddamVamsi264/Comparing-Multiple-Machine-Learning-Models/blob/main/Comparing_Machine_Learning_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Process for Comparing Multiple Machine Learning Models**  \n",
        "\n",
        "When comparing multiple machine learning models, the goal is to identify the most effective algorithm that balances **accuracy, complexity, and performance** for a given problem. The following structured approach can be used:  \n",
        "\n",
        "1. **Data Preprocessing** – Handle missing values, eliminate duplicates, and correct errors to ensure the dataset is clean and reliable.  \n",
        "2. **Dataset Splitting** – Divide the data into **training and testing sets** (commonly **70-30%** or **80-20%**) to evaluate model performance effectively.  \n",
        "3. **Model Selection** – Choose a diverse range of models, including **linear models, tree-based algorithms, ensemble methods,** and **advanced techniques**, based on data characteristics and problem complexity.  \n",
        "4. **Model Training** – Train each selected model on the **training dataset**, allowing it to learn from the input features and target variable.  \n",
        "5. **Performance Evaluation** – Assess each model using **relevant metrics** on the **test dataset** to determine effectiveness.  \n",
        "6. **Comparison & Selection** – Compare models based on **accuracy, computational efficiency, and overall performance** to select the best-suited approach.  \n",
        "\n",
        "This systematic approach helps in making informed decisions when selecting a machine learning model tailored to specific use cases."
      ],
      "metadata": {
        "id": "9CQlaSx0Ep2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s get started with the task of training and comparing multiple Machine Learning models by importing the necessary Python libraries"
      ],
      "metadata": {
        "id": "9455QkiaEuk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data =pd.read_csv('Real_Estate.csv')\n",
        "data_head=data.head()\n",
        "print(data_head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrrxD3CVDkFk",
        "outputId": "6af4e683-e6a3-44e9-ac7e-17596c5f08aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Transaction date  House age  Distance to the nearest MRT station  \\\n",
            "0  2012-09-02 16:42:30.519336       13.3                            4082.0150   \n",
            "1  2012-09-04 22:52:29.919544       35.5                             274.0144   \n",
            "2  2012-09-05 01:10:52.349449        1.1                            1978.6710   \n",
            "3  2012-09-05 13:26:01.189083       22.2                            1055.0670   \n",
            "4  2012-09-06 08:29:47.910523        8.5                             967.4000   \n",
            "\n",
            "   Number of convenience stores   Latitude   Longitude  \\\n",
            "0                             8  25.007059  121.561694   \n",
            "1                             2  25.012148  121.546990   \n",
            "2                            10  25.003850  121.528336   \n",
            "3                             5  24.962887  121.482178   \n",
            "4                             6  25.011037  121.479946   \n",
            "\n",
            "   House price of unit area  \n",
            "0                  6.488673  \n",
            "1                 24.970725  \n",
            "2                 26.694267  \n",
            "3                 38.091638  \n",
            "4                 21.654710  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXDoQW-GE-OQ",
        "outputId": "ce527aff-1f19-446b-9502-0ff890ae065a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 414 entries, 0 to 413\n",
            "Data columns (total 7 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   Transaction date                     414 non-null    object \n",
            " 1   House age                            414 non-null    float64\n",
            " 2   Distance to the nearest MRT station  414 non-null    float64\n",
            " 3   Number of convenience stores         414 non-null    int64  \n",
            " 4   Latitude                             414 non-null    float64\n",
            " 5   Longitude                            414 non-null    float64\n",
            " 6   House price of unit area             414 non-null    float64\n",
            "dtypes: float64(5), int64(1), object(1)\n",
            "memory usage: 22.8+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 414 entries and 7 columns, with no missing values. Here’s a brief overview of the columns:\n",
        "\n",
        "**Transaction date**: The date of the house sale (object type, which suggests it might need conversion or extraction of useful features like year, month, etc.).\n",
        "\n",
        "**House age**: The age of the house in years (float).\n",
        "Distance to the nearest MRT station: The distance to the nearest mass rapid transit station in meters (float).\n",
        "\n",
        "**Number of convenience stores**: The number of convenience stores in the living circle on foot (integer).\n",
        "\n",
        "**Latitude**: The geographic coordinate that specifies the north-south position (float).\n",
        "\n",
        "**Longitude**: The geographic coordinate that specifies the east-west position (float).\n",
        "\n",
        "**House price of unit area**: Price of the house per unit area (float), which is likely our target variable for prediction."
      ],
      "metadata": {
        "id": "khnMb8IUFKXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "\n",
        "#Convert \"Transaction date\" to datetime and extract year and month\n",
        "data['Transaction date'] = pd.to_datetime(data['Transaction date'])\n",
        "data['Transaction Year'] = data['Transaction date'].dt.year\n",
        "data['Transaction Month'] = data['Transaction date'].dt.month\n",
        "\n",
        "#drop the original \"Transaction date\" as we have extracted relevant features\n",
        "data=data.drop(columns=['Transaction date'])\n",
        "\n",
        "#define features adn target variable\n",
        "X=data.drop('House price of unit area', axis=1)\n",
        "y=data['House price of unit area']\n",
        "\n",
        "#split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#scale the feautures\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cY-7uH-FL9M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUFu6L2XHlo4",
        "outputId": "8d976455-84ec-43b3-cad0-c85d0084f1e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(331, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBtxhy8SHrAc",
        "outputId": "54e3789b-5a6d-421e-ff53-fab978e26f4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Commonly Used Models for Regression Tasks**  \n",
        "\n",
        "To compare regression models effectively, we begin with a selection of commonly used approaches:  \n",
        "\n",
        "- **Linear Regression** – A fundamental baseline model for regression problems.  \n",
        "- **Decision Tree Regressor** – A simple tree-based model that captures non-linear relationships.  \n",
        "- **Random Forest Regressor** – An ensemble method that enhances the performance of decision trees by averaging multiple trees.  \n",
        "- **Gradient Boosting Regressor** – A powerful boosting-based ensemble technique that improves predictive accuracy.  \n",
        "\n",
        "Each model will be trained on the **training dataset** and evaluated on the **test dataset** using **Mean Absolute Error (MAE)** and **R-squared (R²)** as performance metrics. These metrics provide insights into the **average prediction error** and the **model’s ability to explain variance in the target variable**."
      ],
      "metadata": {
        "id": "ApIpREYpH3Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "#intializa the models\n",
        "models={\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "#dictionery to hold the evaluation metrics of each model\n",
        "\n",
        "results = {}\n",
        "\n",
        "#train and evaluate each model\n",
        "\n",
        "for name, model in models.items():\n",
        "\n",
        "  #training the model\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  #making predictions on the test set\n",
        "  predictions = model.predict(X_test_scaled)\n",
        "\n",
        "  #evaluating the model\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "\n",
        "  #storing the results in the dictionary\n",
        "  results[name] = {'MAE': mae, 'R2': r2}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2XHrHzbIGNj",
        "outputId": "2aa17673-fd10-41ba-8e35-ecaae3af4832"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   MAE        R2\n",
            "Linear Regression             9.748246  0.529615\n",
            "Decision Tree Regressor      11.760342  0.204962\n",
            "Random Forest Regressor       9.887601  0.509547\n",
            "Gradient Boosting Regressor  10.000117  0.476071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7a5TdydzODNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Performance Comparison**  \n",
        "\n",
        "Based on the evaluation metrics, **Linear Regression** emerges as the **best-performing model**, achieving:  \n",
        "- **Lowest Mean Absolute Error (MAE):** **9.75**  \n",
        "- **Highest R-squared (R²):** **0.53**  \n",
        "\n",
        "This suggests that, despite its simplicity, **Linear Regression effectively captures the relationships in the dataset.**  \n",
        "\n",
        "#### **Performance of Other Models:**  \n",
        "- **Decision Tree Regressor**  \n",
        "  - **Highest MAE:** **11.76**  \n",
        "  - **Lowest R²:** **0.20**  \n",
        "  - Likely overfitting to the training data, leading to poor generalization on the test set.  \n",
        "\n",
        "- **Random Forest Regressor**  \n",
        "  - **MAE:** **9.89**  \n",
        "  - **R²:** **0.51**  \n",
        "  - Performs slightly worse than Linear Regression but significantly better than Decision Tree.  \n",
        "\n",
        "- **Gradient Boosting Regressor**  \n",
        "  - **MAE:** **10.00**  \n",
        "  - **R²:** **0.48**  \n",
        "  - Similar performance to Random Forest, though not as effective as Linear Regression.  \n",
        "\n",
        "### **Conclusion:**  \n",
        "**Linear Regression** provides the best balance of accuracy and generalization for this dataset. **Decision Tree Regressor** struggles with overfitting, while **Random Forest and Gradient Boosting** offer moderate improvements but do not outperform Linear Regression."
      ],
      "metadata": {
        "id": "YHKf76uZJylS"
      }
    }
  ]
}